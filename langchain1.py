"""
Arytic - Intelligent RAG + Multi-Agent Resume System
Version 10.0 - Fixed extraction, ranking, and parallel processing
"""

from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import uvicorn
import os
import uuid
from datetime import datetime
import json
import re
import asyncio
from concurrent.futures import ThreadPoolExecutor

# LangChain imports
from langchain_ollama import ChatOllama, OllamaEmbeddings

# Qdrant imports
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

# Document processing
import PyPDF2
import docx

# ============== CONFIGURATION ==============
OLLAMA_MODEL = "llama3.2:3b"
OLLAMA_BASE_URL = "http://localhost:11434"
EMBEDDING_MODEL = "nomic-embed-text:latest"
QDRANT_URL = "http://localhost:6333"
COLLECTION_NAME = "arytic_resumes"
UPLOAD_DIR = "arytic_uploads"

CPU_THREADS = 4
MAX_CONCURRENT_UPLOADS = 5
MAX_WORKERS = 5
CONTEXT_SIZE = 4096

os.makedirs(UPLOAD_DIR, exist_ok=True)
executor = ThreadPoolExecutor(max_workers=MAX_WORKERS)

# ============== FASTAPI SETUP ==============
app = FastAPI(title="Arytic - Intelligent RAG System")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============== INITIALIZE SERVICES ==============
qdrant_client = QdrantClient(url=QDRANT_URL)
embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)

try:
    llm = ChatOllama(
        model=OLLAMA_MODEL,
        temperature=0.1,
    )
    print("‚úÖ LLM initialized successfully")
except Exception as e:
    print(f"‚ö†Ô∏è  LLM initialization warning: {e}")
    llm = ChatOllama(model=OLLAMA_MODEL)

def initialize_qdrant():
    try:
        collections = qdrant_client.get_collections().collections
        if not any(c.name == COLLECTION_NAME for c in collections):
            test_emb = embeddings.embed_query("test")
            VECTOR_SIZE = len(test_emb)
            qdrant_client.create_collection(
                collection_name=COLLECTION_NAME,
                vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.COSINE),
            )
            print(f"‚úÖ Created collection (vector size: {VECTOR_SIZE})")
        else:
            print(f"‚úÖ Using existing collection")
    except Exception as e:
        print(f"‚ùå Qdrant error: {e}")

initialize_qdrant()

# ============== PYDANTIC MODELS ==============
class ChatRequest(BaseModel):
    message: str

class ChatResponse(BaseModel):
    response: str
    candidates_found: int
    reasoning_steps: List[str]

class UploadResponse(BaseModel):
    status: str
    processed: int
    resumes: List[Dict[str, Any]]

# ============== AGENT SYSTEM ==============

class AgentPersona:
    """Define agent persona with role, goal, and backstory"""
    def __init__(self, role: str, goal: str, backstory: str):
        self.role = role
        self.goal = goal
        self.backstory = backstory
    
    def get_system_prompt(self) -> str:
        """Generate system prompt from persona"""
        return f"""You are a {self.role}.

GOAL: {self.goal}

BACKSTORY: {self.backstory}

You always think step-by-step and provide clear reasoning.
"""

# ============== EXTRACTION ==============

def extract_text_from_pdf(file_path: str) -> str:
    try:
        with open(file_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            return "".join([page.extract_text() for page in pdf_reader.pages])
    except:
        return ""

def extract_text_from_docx(file_path: str) -> str:
    try:
        doc = docx.Document(file_path)
        return "\n".join([para.text for para in doc.paragraphs])
    except:
        return ""

def extract_metadata_with_llm(text: str, filename: str) -> dict:
    """
    STRICT EXTRACTION: Only extract what's explicitly in the resume
    NO hallucination, NO assumptions, NO invented data
    """
    
    extraction_agent = AgentPersona(
        role="Expert Resume Data Extraction Specialist",
        goal="Extract ONLY information that is EXPLICITLY written in resumes with ZERO hallucination or assumptions",
        backstory="""You are an expert at precise data extraction from resumes. You NEVER invent or assume information.

CRITICAL RULES YOU ALWAYS FOLLOW:
1. ONLY extract information that is EXPLICITLY written in the resume text
2. If information is not present, use empty string "" or empty list []
3. NEVER infer locations from university names unless explicitly stated
4. NEVER assume current location from education or past work
5. Extract ALL skills from technical skills sections - every single one
6. Extract certifications ONLY if explicitly mentioned (PMP, AWS, etc.)
7. Calculate years of experience ONLY from explicit dates or statements
8. For locations, ONLY use what's in address, work locations, or explicitly stated education locations

You are thorough but NEVER creative. You extract, you don't invent."""
    )
    
    prompt = f"""{extraction_agent.get_system_prompt()}

CRITICAL INSTRUCTIONS - READ CAREFULLY:
1. ONLY extract information EXPLICITLY present in the resume
2. DO NOT infer, assume, or invent ANY information
3. If something is not clearly stated, leave it as empty string "" or empty array []
4. DO NOT add "Tokyo" or any location unless it's explicitly written in the resume
5. Extract EVERY skill from technical skills section
6. Extract certifications ONLY if they are explicitly listed

RESUME TEXT:
{text[:15000]}

Extract in this EXACT JSON format with NO invented data:

{{
    "name": "ONLY the full name if explicitly stated, otherwise empty",
    "email": "ONLY email if present in text",
    "phone": "ONLY phone if present in text",
    "current_location": "ONLY current location from header/contact section",
    "all_locations": ["ONLY locations explicitly mentioned in work addresses or education cities"],
    "current_role": "ONLY current or most recent job title if stated",
    "years_experience": "ONLY if explicitly stated (e.g., '10+ years') or calculable from work dates",
    "skills": ["Extract EVERY technical skill mentioned - programming languages, tools, technologies"],
    "languages": ["ONLY spoken/programming languages if explicitly mentioned"],
    "certifications": ["ONLY certifications if explicitly listed - PMP, AWS Certified, etc."],
    "education": [
        {{
            "degree": "degree name ONLY if stated",
            "university": "university name ONLY as written",
            "location": "university location ONLY if explicitly stated",
            "year": "graduation year ONLY if stated"
        }}
    ],
    "work_experience": [
        {{
            "company": "company name",
            "role": "job title",
            "location": "work location ONLY if stated",
            "duration": "employment period",
            "responsibilities": "brief summary",
            "technologies": ["technologies mentioned ONLY for this job"]
        }}
    ]
}}

REMEMBER: If you're unsure, leave it empty. NEVER invent data.

Your JSON response:
"""
    
    try:
        response = llm.invoke(prompt).content
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        
        if json_match:
            metadata = json.loads(json_match.group())
            
            # Clean up any potential hallucinations
            if not text or len(text) < 100:
                metadata = {
                    "name": filename.rsplit('.', 1)[0],
                    "raw_text": text[:500],
                    "skills": [],
                    "all_locations": []
                }
        else:
            metadata = {
                "name": filename.rsplit('.', 1)[0],
                "raw_text": text[:500],
                "skills": [],
                "all_locations": []
            }
    except Exception as e:
        print(f"Extraction error: {e}")
        metadata = {
            "name": filename.rsplit('.', 1)[0],
            "raw_text": text[:500],
            "skills": [],
            "all_locations": []
        }
    
    # Add metadata
    metadata['filename'] = filename
    metadata['id'] = str(uuid.uuid4())
    metadata['uploaded_at'] = datetime.utcnow().isoformat()
    
    return metadata


# ============== QUERY PROCESSING ==============

def semantic_search(query: str, limit: int = 20) -> List[Dict]:
    """Perform semantic search"""
    try:
        query_embedding = embeddings.embed_query(query)
        search_results = qdrant_client.search(
            collection_name=COLLECTION_NAME,
            query_vector=query_embedding,
            limit=limit
        )
        
        # Deduplicate results by name and email
        candidates = []
        seen = set()
        
        for hit in search_results:
            candidate = hit.payload
            
            # Create unique key
            name = candidate.get('name', '').strip().lower()
            email = candidate.get('email', '').strip().lower()
            unique_key = f"{name}-{email}"
            
            if unique_key not in seen and name:  # Only add if has name
                seen.add(unique_key)
                candidate['_search_score'] = hit.score
                candidates.append(candidate)
        
        return candidates
    except Exception as e:
        print(f"Search error: {e}")
        return []


def rank_candidates_with_cot(query: str, candidates: List[Dict]) -> str:
    """
    Rank candidates using Chain-of-Thought reasoning
    """
    
    ranking_agent = AgentPersona(
        role="Senior Technical Recruiter with 20+ years experience",
        goal="Rank candidates by relevance to the job requirements with detailed reasoning",
        backstory="""You have placed 5000+ candidates and understand:
        - Exact role matches are most important
        - Years of experience matter significantly  
        - Skills match is critical - check for required technologies
        - Location can indicate regional experience
        - Education and certifications boost credibility
        
You provide honest, detailed scoring with clear reasoning."""
    )
    
    # Enhance candidates with role matching
    query_lower = query.lower()
    for candidate in candidates:
        candidate_role = candidate.get('current_role', '').lower()
        
        # Calculate role match
        if 'data engineer' in query_lower and 'data engineer' in candidate_role:
            candidate['_role_match'] = 'EXACT'
        elif 'engineer' in query_lower and 'engineer' in candidate_role:
            candidate['_role_match'] = 'PARTIAL'
        elif 'project manager' in query_lower and 'project manager' in candidate_role:
            candidate['_role_match'] = 'EXACT'
        elif 'manager' in query_lower and 'manager' in candidate_role:
            candidate['_role_match'] = 'PARTIAL'
        elif 'developer' in query_lower and 'developer' in candidate_role:
            candidate['_role_match'] = 'EXACT'
        else:
            candidate['_role_match'] = 'NONE'
    
    # Sort by role match first
    candidates_sorted = sorted(
        candidates,
        key=lambda x: (
            1 if x.get('_role_match') == 'EXACT' else (0.5 if x.get('_role_match') == 'PARTIAL' else 0),
            x.get('years_experience', 0) if isinstance(x.get('years_experience'), (int, float)) else 0
        ),
        reverse=True
    )
    
    # Format candidates for analysis
    candidates_summary = []
    for i, c in enumerate(candidates_sorted[:10], 1):
        summary = f"""
Candidate {i}: {c.get('name', 'Unknown')}
- Current Role: {c.get('current_role', 'Not specified')}
- Role Match: {c.get('_role_match', 'UNKNOWN')} {'‚≠ê‚≠ê‚≠ê EXCELLENT MATCH' if c.get('_role_match') == 'EXACT' else ''}
- Experience: {c.get('years_experience', 'Not specified')} years
- Skills: {', '.join(c.get('skills', [])[:10])}
- Certifications: {', '.join(c.get('certifications', []))}
- Location: {c.get('current_location', 'Not specified')}
- Email: {c.get('email', 'Not provided')}
- Phone: {c.get('phone', 'Not provided')}
"""
        candidates_summary.append(summary)
    
    prompt = f"""{ranking_agent.get_system_prompt()}

USER QUERY: {query}

CANDIDATES TO RANK (Pre-sorted by role relevance):
{''.join(candidates_summary)}

RANKING INSTRUCTIONS:
1. EXACT role matches should score 90+ (these are priority)
2. Consider years of experience (more is better for senior roles)
3. Skills match - do they have required technologies?
4. Certifications add credibility
5. Provide detailed reasoning for each candidate

Create a response with:
1. Top 3-5 candidates ranked by fit
2. Clear explanation of why each matches
3. Contact information for each
4. Overall recommendation

Your response:
"""
    
    try:
        response = llm.invoke(prompt).content
        return response
    except Exception as e:
        print(f"Ranking error: {e}")
        return f"Found {len(candidates)} candidates. Please review results."


# ============== PARALLEL UPLOAD PROCESSING ==============

async def process_single_file(file: UploadFile) -> Optional[Dict[str, Any]]:
    """Process a single file with extraction and storage"""
    try:
        # Save file
        file_path = os.path.join(UPLOAD_DIR, file.filename)
        content = await file.read()
        
        # Save to disk (run in executor to not block)
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(
            executor,
            lambda: open(file_path, 'wb').write(content)
        )
        
        # Extract text
        if file.filename.endswith('.pdf'):
            text = await loop.run_in_executor(executor, extract_text_from_pdf, file_path)
        elif file.filename.endswith(('.docx', '.doc')):
            text = await loop.run_in_executor(executor, extract_text_from_docx, file_path)
        else:
            print(f"Unsupported file type: {file.filename}")
            return None
        
        # Extract metadata
        metadata = await loop.run_in_executor(
            executor,
            extract_metadata_with_llm,
            text,
            file.filename
        )
        
        # Create embedding
        embedding_text = f"""
Name: {metadata.get('name', '')}
Role: {metadata.get('current_role', '')}
Location: {', '.join(metadata.get('all_locations', []))}
Skills: {', '.join(metadata.get('skills', [])[:30])}
Experience: {metadata.get('years_experience', '')} years
Certifications: {', '.join(metadata.get('certifications', []))}
Education: {json.dumps(metadata.get('education', []))}
"""
        
        embedding = await loop.run_in_executor(
            executor,
            embeddings.embed_query,
            embedding_text
        )
        
        # Store in Qdrant
        point = PointStruct(
            id=metadata['id'],
            vector=embedding,
            payload=metadata
        )
        
        await loop.run_in_executor(
            executor,
            lambda: qdrant_client.upsert(
                collection_name=COLLECTION_NAME,
                points=[point]
            )
        )
        
        print(f"‚úÖ Processed: {file.filename}")
        return metadata
        
    except Exception as e:
        print(f"‚ùå Error processing {file.filename}: {e}")
        return None


# ============== API ENDPOINTS ==============

@app.post("/upload", response_model=UploadResponse)
async def upload_resumes(files: List[UploadFile] = File(...)):
    """
    Upload and process multiple resumes in parallel
    """
    print(f"\nüì§ Starting upload of {len(files)} files...")
    
    # Process files in parallel (with semaphore to limit concurrency)
    semaphore = asyncio.Semaphore(MAX_CONCURRENT_UPLOADS)
    
    async def process_with_limit(file):
        async with semaphore:
            return await process_single_file(file)
    
    tasks = [process_with_limit(file) for file in files]
    results = await asyncio.gather(*tasks)
    
    # Filter out None results
    processed_resumes = [r for r in results if r is not None]
    
    print(f"‚úÖ Successfully processed {len(processed_resumes)}/{len(files)} files")
    
    return UploadResponse(
        status="success",
        processed=len(processed_resumes),
        resumes=processed_resumes
    )


@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    Query candidates with intelligent ranking
    """
    try:
        print(f"\nüîç Query: {request.message}")
        
        # Step 1: Semantic search
        candidates = semantic_search(request.message, limit=30)
        print(f"üìä Found {len(candidates)} unique candidates")
        
        if not candidates:
            return ChatResponse(
                response="No candidates found matching your query. Please try different keywords or upload more resumes.",
                candidates_found=0,
                reasoning_steps=["No candidates found in database"]
            )
        
        # Step 2: Rank with Chain-of-Thought
        ranked_response = rank_candidates_with_cot(request.message, candidates)
        
        reasoning_steps = [
            f"Semantic search found {len(candidates)} unique candidates",
            "Ranked by role match, experience, and skills",
            "Applied Chain-of-Thought reasoning for recommendations"
        ]
        
        return ChatResponse(
            response=ranked_response,
            candidates_found=len(candidates),
            reasoning_steps=reasoning_steps
        )
        
    except Exception as e:
        print(f"‚ùå Chat error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/stats")
async def get_stats():
    """Get system statistics"""
    try:
        collection_info = qdrant_client.get_collection(COLLECTION_NAME)
        return {
            "total_resumes": collection_info.points_count,
            "collection_name": COLLECTION_NAME,
            "model": OLLAMA_MODEL,
            "embedding_model": EMBEDDING_MODEL,
            "status": "operational"
        }
    except:
        return {"total_resumes": 0, "status": "error"}


@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "system": "Arytic Intelligent Resume System",
        "version": "10.0 - Fixed",
        "features": [
            "Zero hallucination extraction",
            "Intelligent role-based ranking",
            "Parallel resume processing",
            "Deduplication",
            "Chain-of-Thought reasoning"
        ],
        "endpoints": {
            "POST /upload": "Upload resumes",
            "POST /chat": "Query candidates",
            "GET /stats": "System statistics"
        }
    }


@app.on_event("startup")
async def startup_event():
    """Print startup information"""
    print("\n" + "="*70)
    print("üöÄ ARYTIC INTELLIGENT RESUME SYSTEM - FIXED VERSION")
    print("="*70)
    print("\n‚úÖ FIXES APPLIED:")
    print("   1. ‚úÖ Zero hallucination extraction (no fake Tokyo)")
    print("   2. ‚úÖ Complete field extraction (role, experience, certifications)")
    print("   3. ‚úÖ Deduplication (no duplicate candidates)")
    print("   4. ‚úÖ Smart ranking (exact role matches prioritized)")
    print("   5. ‚úÖ Parallel processing (faster uploads)")
    print(f"\nüîß Configuration:")
    print(f"   ‚Ä¢ Model: {OLLAMA_MODEL}")
    print(f"   ‚Ä¢ Embedding: {EMBEDDING_MODEL}")
    print(f"   ‚Ä¢ Max concurrent uploads: {MAX_CONCURRENT_UPLOADS}")
    print("\n" + "="*70)
    print("üåê Server: http://127.0.0.1:8000")
    print("üìö API Docs: http://127.0.0.1:8000/docs")
    print("="*70 + "\n")


if __name__ == "__main__":
    uvicorn.run(
        app,
        host="127.0.0.1",
        port=8000,
        log_level="info"
    )